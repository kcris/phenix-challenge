# Phenix Challenge


## Solution

#### In memory structure

for a given day J we have this structure called (daily) journal
 	map<storeId, productsGroup>

where productsGroup contains
	map<productId, productInfo>

where productInfo contains
	productId, unitsSold, valueSold

The main idea is to map-reduce the available input data.

unitsSold - is used to determine the volume of sales for a product (ventes)
valueSold - is used to determine the value of sales for a product (chiffre affaires)

####  Output statistics

For a given day J
- to compute top ventes/ca per store: for a given store, keep top 100 products (based on unitsSold/valueSold)
- to compute top ventes/ca globally: we *merge* all productsGroup, then keep top 100 products (based on unitsSold/valueSold)

For a given week J1..J7
- to compute top ventes/ca per store: for a given store, *merge* the 7-day productsGroup, then keep top 100 (based on unitsSold/valueSold) 
- to compute top ventes/ca globally: *merge* the 7-day productsGroup of all stores, then keep top 100 (based on unitsSold/valueSold) 

#### Technical details

- plain java application, minimal or no dependencies
	- no external libs except log4j (and that's not important either)
- large input data
	- csv is not fully loaded in memory, useless fields are ignored
	- is 'reduced' while loading to 1 entry per productId
- comparators used to parameterize the way that *top* is calculated
	- by sales volume (consider a productInfo's unitsSold)
	- by sales value (consider a productInfo's valueSold)
- aggregators supported to parameterize the way that 2 productInfos instances are merged
	- current aggregation policy: sums the unitsSold and valueSold (for the same productId)
	- useful to merge multiple productInfo coming from different stores when computing the global top
	- useful to merge weekly productInfo coming from different days (and different stores) when computing the weekly (global) top
- main work was split into 2 phases
	- phase A: create journals for last week (7 days) - this was split into 2 threads
		- one thread loads journal for 3 days
		- one thread loads journal for 4 days
	- phase B: generate the output statistics - this was also split in 2 threads
		- one thread generates volume stats daily(per store + global) and weekly (per store+global)
		- one thread generates value stats daily(per store + global) and weekly (per store+global)
	- phase B depends on phase A, they are serialized
	- the load for each phase's 2 threads is equally distributed

#### TODOs

- tests
	- more unit tests
	- full-scale test data set
- optimize
	- intermediary files for smaller batch processing?
	- reuse oututs generated by previous runs?
- improve
	- configuration support (args or conf file)
	- more docs


## Running the application

```bash
cd carrefour.dsi

#re-build application via maven if needed
mvn clean package
cp phenix-challenge/target/phenix-challenge-0.0.1-SNAPSHOT.jar phenix-challenge/lib/

#run the application
java -Xmx512M -cp "phenix-challenge/lib/*" fr.carrefour.phenix.challenge.Application

#inputs taken from folder: phenix-challenge/data/input
#outputs sent to folder: phenix-challenge/data/output/<date>
```
